{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_NRhpjUyeI5"
   },
   "source": [
    "## Custom functions definition:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BWlcfanNynPk",
    "outputId": "9fd78029-529f-468e-d8f2-ed561f5e6ad2"
   },
   "source": [
    "# from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FTNPOxIJzbYF",
    "outputId": "45def65e-4040-424a-c00c-22b3ce5636b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in c:\\users\\appir\\anaconda3\\lib\\site-packages (2.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\appir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\appir\\anaconda3\\lib\\site-packages (4.28.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in c:\\users\\appir\\anaconda3\\lib\\site-packages (from transformers) (0.14.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\appir\\anaconda3\\lib\\site-packages (from transformers) (21.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\appir\\anaconda3\\lib\\site-packages (from transformers) (3.3.1)\n",
      "Requirement already satisfied: requests in c:\\users\\appir\\anaconda3\\lib\\site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\appir\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\appir\\anaconda3\\lib\\site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\appir\\anaconda3\\lib\\site-packages (from transformers) (2021.8.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\appir\\anaconda3\\lib\\site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\appir\\anaconda3\\lib\\site-packages (from transformers) (1.20.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\appir\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2021.10.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\appir\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\appir\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\appir\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\appir\\anaconda3\\lib\\site-packages (from requests->transformers) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\appir\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\appir\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\appir\\anaconda3\\lib\\site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: demoji in c:\\users\\appir\\anaconda3\\lib\\site-packages (1.1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\appir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install emoji\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "!pip install transformers\n",
    "!pip install demoji\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6fTGnupKyeI_"
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "execution": {
     "iopub.execute_input": "2022-03-15T11:06:42.394589Z",
     "iopub.status.busy": "2022-03-15T11:06:42.394161Z",
     "iopub.status.idle": "2022-03-15T11:06:45.309727Z",
     "shell.execute_reply": "2022-03-15T11:06:45.308901Z",
     "shell.execute_reply.started": "2022-03-15T11:06:42.394553Z"
    },
    "id": "D0GOzHmK5QsH",
    "outputId": "b23f2206-2a5d-43ca-f940-ea73072e5ac5"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'imblearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_26124/2457598532.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mimblearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mover_sampling\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomOverSampler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m#Naive Bayes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'imblearn'"
     ]
    }
   ],
   "source": [
    "#Libraries for general purpose\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#Text cleaning\n",
    "import re, string\n",
    "import emoji\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "#Data preprocessing\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "#Naive Bayes\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "#PyTorch LSTM\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "#Tokenization for LSTM\n",
    "from collections import Counter\n",
    "#from gensim.models import Word2Vec\n",
    "\n",
    "#Transformers library for BERT\n",
    "import transformers\n",
    "from transformers import BertModel\n",
    "from transformers import BertTokenizer\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "#Seed for reproducibility\n",
    "import random\n",
    "\n",
    "seed_value=42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "torch.manual_seed(seed_value)\n",
    "torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "import time\n",
    "\n",
    "#set style for plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.despine()\n",
    "plt.style.use(\"seaborn-whitegrid\")\n",
    "plt.rc(\"figure\", autolayout=True)\n",
    "plt.rc(\"axes\", labelweight=\"bold\", labelsize=\"large\", titleweight=\"bold\", titlepad=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z_Edch4JyeJB"
   },
   "source": [
    "## Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T11:06:45.314007Z",
     "iopub.status.busy": "2022-03-15T11:06:45.313239Z",
     "iopub.status.idle": "2022-03-15T11:06:45.427226Z",
     "shell.execute_reply": "2022-03-15T11:06:45.426478Z",
     "shell.execute_reply.started": "2022-03-15T11:06:45.313964Z"
    },
    "id": "aCVJ1XJi5QsI"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\appir\\Desktop\\MAIN PROJECT/cyberbullying2.csv\",encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "execution": {
     "iopub.execute_input": "2022-03-15T11:06:45.430571Z",
     "iopub.status.busy": "2022-03-15T11:06:45.43035Z",
     "iopub.status.idle": "2022-03-15T11:06:45.442208Z",
     "shell.execute_reply": "2022-03-15T11:06:45.441427Z",
     "shell.execute_reply.started": "2022-03-15T11:06:45.430546Z"
    },
    "id": "NPhe4rM35QsJ",
    "outputId": "4cce6c96-2447-4531-fdc0-eb1449e0d534"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-03-15T11:06:45.444447Z",
     "iopub.status.busy": "2022-03-15T11:06:45.44357Z",
     "iopub.status.idle": "2022-03-15T11:06:45.465481Z",
     "shell.execute_reply": "2022-03-15T11:06:45.464746Z",
     "shell.execute_reply.started": "2022-03-15T11:06:45.44441Z"
    },
    "id": "2zXXKDYwyeJD",
    "outputId": "16b053c8-8a16-49a7-d7c6-e2722e5b33a7"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EM344ZPiyeJD"
   },
   "source": [
    "First we rename the columns using shorter words for easier reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T11:06:45.467426Z",
     "iopub.status.busy": "2022-03-15T11:06:45.466912Z",
     "iopub.status.idle": "2022-03-15T11:06:45.472765Z",
     "shell.execute_reply": "2022-03-15T11:06:45.472018Z",
     "shell.execute_reply.started": "2022-03-15T11:06:45.467388Z"
    },
    "id": "AwUN352x5QsK"
   },
   "outputs": [],
   "source": [
    "df = df.rename(columns={'tweet_text': 'text', 'cyberbullying_type': 'sentiment'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u9wTHjTcc1D7"
   },
   "source": [
    "### Are there duplicated tweets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-03-15T11:06:45.474658Z",
     "iopub.status.busy": "2022-03-15T11:06:45.474397Z",
     "iopub.status.idle": "2022-03-15T11:06:45.513341Z",
     "shell.execute_reply": "2022-03-15T11:06:45.512638Z",
     "shell.execute_reply.started": "2022-03-15T11:06:45.474625Z"
    },
    "id": "fMIsLnAoyeJE",
    "outputId": "25fb14ab-801c-4825-e277-71d37672e6b7"
   },
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3uGBmMdWyeJF"
   },
   "source": [
    "There are some duplicated tweets, we will remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T11:06:45.514838Z",
     "iopub.status.busy": "2022-03-15T11:06:45.514494Z",
     "iopub.status.idle": "2022-03-15T11:06:45.546538Z",
     "shell.execute_reply": "2022-03-15T11:06:45.545845Z",
     "shell.execute_reply.started": "2022-03-15T11:06:45.514797Z"
    },
    "id": "4u7mn7wkchxj"
   },
   "outputs": [],
   "source": [
    "df = df[~df.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-03-15T11:06:45.548284Z",
     "iopub.status.busy": "2022-03-15T11:06:45.547843Z",
     "iopub.status.idle": "2022-03-15T11:06:45.567784Z",
     "shell.execute_reply": "2022-03-15T11:06:45.566278Z",
     "shell.execute_reply.started": "2022-03-15T11:06:45.548249Z"
    },
    "id": "oedUGlQ9crg2",
    "outputId": "9b7d7734-c609-40ce-fdd4-a791746a5062"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s0AkaHhdyeJF"
   },
   "source": [
    "### Are the classes balanced?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-03-15T11:06:45.570493Z",
     "iopub.status.busy": "2022-03-15T11:06:45.570171Z",
     "iopub.status.idle": "2022-03-15T11:06:45.584722Z",
     "shell.execute_reply": "2022-03-15T11:06:45.58406Z",
     "shell.execute_reply.started": "2022-03-15T11:06:45.570457Z"
    },
    "id": "c82luV2N5QsN",
    "outputId": "6f59388f-f43f-4688-b909-b21905153be9"
   },
   "outputs": [],
   "source": [
    "df.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JfNip_qFyeJG"
   },
   "source": [
    "The classes look balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-19T09:34:52.112295Z",
     "iopub.status.busy": "2022-01-19T09:34:52.112005Z",
     "iopub.status.idle": "2022-01-19T09:34:53.14386Z",
     "shell.execute_reply": "2022-01-19T09:34:53.142913Z",
     "shell.execute_reply.started": "2022-01-19T09:34:52.112262Z"
    },
    "id": "DGmYrF4d5QsP"
   },
   "source": [
    "# Tweets text deep cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8j6DWGXdyeJH"
   },
   "source": [
    "Next, we will define custom functions to clean the texts of the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T11:06:45.58674Z",
     "iopub.status.busy": "2022-03-15T11:06:45.586233Z",
     "iopub.status.idle": "2022-03-15T11:06:45.604592Z",
     "shell.execute_reply": "2022-03-15T11:06:45.603771Z",
     "shell.execute_reply.started": "2022-03-15T11:06:45.586703Z"
    },
    "id": "E5E1UwMc5QsQ"
   },
   "outputs": [],
   "source": [
    "##CUSTOM DEFINED FUNCTIONS TO CLEAN THE TWEETS\n",
    "import demoji\n",
    "#Clean emojis from text\n",
    "def strip_emoji(text):\n",
    "    return demoji.replace(text, '') #remove emoji\n",
    "\n",
    "#Remove punctuations, links, stopwords, mentions and \\r\\n new line characters\n",
    "def strip_all_entities(text): \n",
    "    text = text.replace('\\r', '').replace('\\n', ' ').lower() #remove \\n and \\r and lowercase\n",
    "    text = re.sub(r\"(?:\\@|https?\\://)\\S+\", \"\", text) #remove links and mentions\n",
    "    text = re.sub(r'[^\\x00-\\x7f]',r'', text) #remove non utf8/ascii characters such as '\\x9a\\x91\\x97\\x9a\\x97'\n",
    "    banned_list= string.punctuation\n",
    "    table = str.maketrans('', '', banned_list)\n",
    "    text = text.translate(table)\n",
    "    text = [word for word in text.split() if word not in stop_words]\n",
    "    text = ' '.join(text)\n",
    "    text =' '.join(word for word in text.split() if len(word) < 14) # remove words longer than 14 characters\n",
    "    return text\n",
    "\n",
    "#remove contractions\n",
    "def decontract(text):\n",
    "    text = re.sub(r\"can\\'t\", \"can not\", text)\n",
    "    text = re.sub(r\"n\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'s\", \" is\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'m\", \" am\", text)\n",
    "    return text\n",
    "\n",
    "#clean hashtags at the end of the sentence, and keep those in the middle of the sentence by removing just the \"#\" symbol\n",
    "def clean_hashtags(tweet):\n",
    "    new_tweet = \" \".join(word.strip() for word in re.split('#(?!(?:hashtag)\\b)[\\w-]+(?=(?:\\s+#[\\w-]+)*\\s*$)', tweet)) #remove last hashtags\n",
    "    new_tweet2 = \" \".join(word.strip() for word in re.split('#|_', new_tweet)) #remove hashtags symbol from words in the middle of the sentence\n",
    "    return new_tweet2\n",
    "\n",
    "#Filter special characters such as \"&\" and \"$\" present in some words\n",
    "def filter_chars(a):\n",
    "    sent = []\n",
    "    for word in a.split(' '):\n",
    "        if ('$' in word) | ('&' in word):\n",
    "            sent.append('')\n",
    "        else:\n",
    "            sent.append(word)\n",
    "    return ' '.join(sent)\n",
    "\n",
    "#Remove multiple sequential spaces\n",
    "def remove_mult_spaces(text):\n",
    "    return re.sub(\"\\s\\s+\" , \" \", text)\n",
    "\n",
    "#Stemming\n",
    "def stemmer(text):\n",
    "    tokenized = nltk.word_tokenize(text)\n",
    "    ps = PorterStemmer()\n",
    "    return ' '.join([ps.stem(words) for words in tokenized])\n",
    "\n",
    "#Lemmatization \n",
    "#NOTE:Stemming seems to work better for this dataset\n",
    "def lemmatize(text):\n",
    "    tokenized = nltk.word_tokenize(text)\n",
    "    lm = WordNetLemmatizer()\n",
    "    return ' '.join([lm.lemmatize(words) for words in tokenized])\n",
    "\n",
    "#Then we apply all the defined functions in the following order\n",
    "def deep_clean(text):\n",
    "    text = strip_emoji(text)\n",
    "    text = decontract(text)\n",
    "    text = strip_all_entities(text)\n",
    "    text = clean_hashtags(text)\n",
    "    text = filter_chars(text)\n",
    "    text = remove_mult_spaces(text)\n",
    "    text = stemmer(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T11:06:45.609192Z",
     "iopub.status.busy": "2022-03-15T11:06:45.608811Z",
     "iopub.status.idle": "2022-03-15T11:07:56.524297Z",
     "shell.execute_reply": "2022-03-15T11:07:56.523492Z",
     "shell.execute_reply.started": "2022-03-15T11:06:45.609158Z"
    },
    "id": "Lx-xXilI5QsT"
   },
   "outputs": [],
   "source": [
    "texts_new = []\n",
    "for t in df.text:\n",
    "    texts_new.append(deep_clean(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T11:07:56.52586Z",
     "iopub.status.busy": "2022-03-15T11:07:56.525598Z",
     "iopub.status.idle": "2022-03-15T11:07:56.538252Z",
     "shell.execute_reply": "2022-03-15T11:07:56.537531Z",
     "shell.execute_reply.started": "2022-03-15T11:07:56.525826Z"
    },
    "id": "Ah-xjjhv5QsT"
   },
   "outputs": [],
   "source": [
    "df['text_clean'] = texts_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "execution": {
     "iopub.execute_input": "2022-03-15T11:07:56.540848Z",
     "iopub.status.busy": "2022-03-15T11:07:56.540277Z",
     "iopub.status.idle": "2022-03-15T11:07:56.55415Z",
     "shell.execute_reply": "2022-03-15T11:07:56.553406Z",
     "shell.execute_reply.started": "2022-03-15T11:07:56.54081Z"
    },
    "id": "zFEEjEBwyeJJ",
    "outputId": "ac2257f1-85ac-4b6c-e85f-5446824a4b5e"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OoxOt-cryeJJ"
   },
   "source": [
    "### Are there duplicate tweets after the cleaning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-03-15T11:07:56.556447Z",
     "iopub.status.busy": "2022-03-15T11:07:56.555757Z",
     "iopub.status.idle": "2022-03-15T11:07:56.563752Z",
     "shell.execute_reply": "2022-03-15T11:07:56.56298Z",
     "shell.execute_reply.started": "2022-03-15T11:07:56.556408Z"
    },
    "id": "hhgyrUQrdICU",
    "outputId": "4da401e7-8e80-4955-ac1f-26a12818e3c3"
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-03-15T11:07:56.565443Z",
     "iopub.status.busy": "2022-03-15T11:07:56.565192Z",
     "iopub.status.idle": "2022-03-15T11:07:56.581683Z",
     "shell.execute_reply": "2022-03-15T11:07:56.581026Z",
     "shell.execute_reply.started": "2022-03-15T11:07:56.565411Z"
    },
    "id": "r5DHDI7QdCfu",
    "outputId": "70f11c9d-e0cc-4dca-9506-6e05ddc5bb8f"
   },
   "outputs": [],
   "source": [
    "df[\"text_clean\"].duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IBpWXd-ayeJK"
   },
   "source": [
    "There are around 3000 duplicated tweets, we will remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T11:07:56.584183Z",
     "iopub.status.busy": "2022-03-15T11:07:56.582713Z",
     "iopub.status.idle": "2022-03-15T11:07:56.611123Z",
     "shell.execute_reply": "2022-03-15T11:07:56.610444Z",
     "shell.execute_reply.started": "2022-03-15T11:07:56.584146Z"
    },
    "id": "zEf4p1jWyeJK"
   },
   "outputs": [],
   "source": [
    "df.drop_duplicates(\"text_clean\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-03-15T11:07:56.612913Z",
     "iopub.status.busy": "2022-03-15T11:07:56.61241Z",
     "iopub.status.idle": "2022-03-15T11:07:56.618206Z",
     "shell.execute_reply": "2022-03-15T11:07:56.617451Z",
     "shell.execute_reply.started": "2022-03-15T11:07:56.612877Z"
    },
    "id": "tdCDSt1PdpXG",
    "outputId": "0afc5780-109e-477f-f143-f9e210f87e4e"
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6rrglDh-yeJL"
   },
   "source": [
    "We removed the duplicated cleaned tweets. How is the class balance after the cleaning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-03-15T11:07:56.620088Z",
     "iopub.status.busy": "2022-03-15T11:07:56.6196Z",
     "iopub.status.idle": "2022-03-15T11:07:56.635857Z",
     "shell.execute_reply": "2022-03-15T11:07:56.635215Z",
     "shell.execute_reply.started": "2022-03-15T11:07:56.620031Z"
    },
    "id": "h-vi7dx5yeJL",
    "outputId": "70dd4faf-031b-404c-bad4-419152c404f4"
   },
   "outputs": [],
   "source": [
    "df.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jkJXN4yKyeJL"
   },
   "source": [
    "We can see that lots of tweets of the class \"other_cyberbullying\" have been removed. Since the class is very unbalanced compared to the other classes and looks too \"generic\", we decide to remove the tweets labeled belonging to this class.<br>\n",
    "EDIT: by performing some tests, the f1 score for predicting the \"other_cyberbullying\" resulted to be around 60%, a value far lower compared to the othter f1 scores (around 95% using LSTM model). This supports the decision of removing this generic class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T11:07:56.638186Z",
     "iopub.status.busy": "2022-03-15T11:07:56.636786Z",
     "iopub.status.idle": "2022-03-15T11:07:56.651611Z",
     "shell.execute_reply": "2022-03-15T11:07:56.650917Z",
     "shell.execute_reply.started": "2022-03-15T11:07:56.638146Z"
    },
    "id": "evV08J3_yeJM"
   },
   "outputs": [],
   "source": [
    "df = df[df[\"sentiment\"]!=\"other_cyberbullying\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CG3LnPmKyeJM"
   },
   "source": [
    "Then we also define a list of the classes names, which will be useful for the future plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T11:07:56.653528Z",
     "iopub.status.busy": "2022-03-15T11:07:56.653218Z",
     "iopub.status.idle": "2022-03-15T11:07:56.658619Z",
     "shell.execute_reply": "2022-03-15T11:07:56.657623Z",
     "shell.execute_reply.started": "2022-03-15T11:07:56.653491Z"
    },
    "id": "ik23aK4RyeJM"
   },
   "outputs": [],
   "source": [
    "sentiments = [\"religion\",\"age\",\"ethnicity\",\"gender\",\"not bullying\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aoLReTpRyeJN"
   },
   "source": [
    "# Tweets length analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ytidrtdGyeJN"
   },
   "source": [
    "Now we will define a new dataframe column containing the length of the cleaned tweets in terms of number of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T11:07:56.660678Z",
     "iopub.status.busy": "2022-03-15T11:07:56.660135Z",
     "iopub.status.idle": "2022-03-15T11:07:56.712752Z",
     "shell.execute_reply": "2022-03-15T11:07:56.712143Z",
     "shell.execute_reply.started": "2022-03-15T11:07:56.660632Z"
    },
    "id": "vQsHVXTA5QsV"
   },
   "outputs": [],
   "source": [
    "text_len = []\n",
    "for text in df.text_clean:\n",
    "    tweet_len = len(text.split())\n",
    "    text_len.append(tweet_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T11:07:56.714376Z",
     "iopub.status.busy": "2022-03-15T11:07:56.713907Z",
     "iopub.status.idle": "2022-03-15T11:07:56.745434Z",
     "shell.execute_reply": "2022-03-15T11:07:56.744809Z",
     "shell.execute_reply.started": "2022-03-15T11:07:56.71434Z"
    },
    "id": "op665CKx5QsV"
   },
   "outputs": [],
   "source": [
    "df['text_len'] = text_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 506
    },
    "execution": {
     "iopub.execute_input": "2022-03-15T11:07:56.746702Z",
     "iopub.status.busy": "2022-03-15T11:07:56.74648Z",
     "iopub.status.idle": "2022-03-15T11:07:57.078482Z",
     "shell.execute_reply": "2022-03-15T11:07:57.077753Z",
     "shell.execute_reply.started": "2022-03-15T11:07:56.746669Z"
    },
    "id": "HZcu8uGh5QsW",
    "outputId": "fa7e5f71-3054-4f1c-bb82-78daafb3735b"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,5))\n",
    "ax = sns.countplot(x='text_len', data=df[df['text_len']<10], palette='mako')\n",
    "plt.title('Count of tweets with less than 10 words', fontsize=20)\n",
    "plt.yticks([])\n",
    "ax.bar_label(ax.containers[0])\n",
    "plt.ylabel('count')\n",
    "plt.xlabel('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ioVoAG8syeJO"
   },
   "source": [
    "We will remove tweets that are too short (less than 4 words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T11:07:57.083518Z",
     "iopub.status.busy": "2022-03-15T11:07:57.082883Z",
     "iopub.status.idle": "2022-03-15T11:07:57.099007Z",
     "shell.execute_reply": "2022-03-15T11:07:57.098259Z",
     "shell.execute_reply.started": "2022-03-15T11:07:57.083474Z"
    },
    "id": "6rEvtAXE5QsX"
   },
   "outputs": [],
   "source": [
    "df = df[df['text_len'] > 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fmN9566u5QsX"
   },
   "source": [
    "### What about long tweets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "execution": {
     "iopub.execute_input": "2022-03-15T11:07:57.100672Z",
     "iopub.status.busy": "2022-03-15T11:07:57.100314Z",
     "iopub.status.idle": "2022-03-15T11:07:57.123723Z",
     "shell.execute_reply": "2022-03-15T11:07:57.123015Z",
     "shell.execute_reply.started": "2022-03-15T11:07:57.100638Z"
    },
    "id": "2Dv2aZRv5QsY",
    "outputId": "d6f36329-dde8-4b1d-9d92-e2ca516cd210"
   },
   "outputs": [],
   "source": [
    "df.sort_values(by=['text_len'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 400
    },
    "execution": {
     "iopub.execute_input": "2022-03-15T11:07:57.125314Z",
     "iopub.status.busy": "2022-03-15T11:07:57.125028Z",
     "iopub.status.idle": "2022-03-15T11:07:58.182193Z",
     "shell.execute_reply": "2022-03-15T11:07:58.181466Z",
     "shell.execute_reply.started": "2022-03-15T11:07:57.125278Z"
    },
    "id": "Zt9YFv935QsY",
    "outputId": "7163dba3-69df-4e0a-fdd6-a0ee79a750d2"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,5))\n",
    "ax = sns.countplot(x='text_len', data=df[(df['text_len']<=1000) & (df['text_len']>10)], palette='Blues_r')\n",
    "plt.title('Count of tweets with high number of words', fontsize=25)\n",
    "plt.yticks([])\n",
    "ax.bar_label(ax.containers[0])\n",
    "plt.ylabel('count')\n",
    "plt.xlabel('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JzY9dDnF5QsY"
   },
   "source": [
    "We also will remove tweets that are too long (with more than 100 words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T11:07:58.186418Z",
     "iopub.status.busy": "2022-03-15T11:07:58.185864Z",
     "iopub.status.idle": "2022-03-15T11:07:58.197824Z",
     "shell.execute_reply": "2022-03-15T11:07:58.196919Z",
     "shell.execute_reply.started": "2022-03-15T11:07:58.186377Z"
    },
    "id": "teb7a02F5QsY"
   },
   "outputs": [],
   "source": [
    "df = df[df['text_len'] < 100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jHGrLYOGyeJQ"
   },
   "source": [
    "Then we also get the length of the longest tweet since it will be useful later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-03-15T11:07:58.200176Z",
     "iopub.status.busy": "2022-03-15T11:07:58.199618Z",
     "iopub.status.idle": "2022-03-15T11:07:58.20753Z",
     "shell.execute_reply": "2022-03-15T11:07:58.206712Z",
     "shell.execute_reply.started": "2022-03-15T11:07:58.200138Z"
    },
    "id": "HyQak-0QI9Mk",
    "outputId": "66b180ed-916a-4d60-cd1b-870cc6b4bbdb"
   },
   "outputs": [],
   "source": [
    "max_len = np.max(df['text_len'])\n",
    "max_len "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "execution": {
     "iopub.execute_input": "2022-03-15T11:07:58.210135Z",
     "iopub.status.busy": "2022-03-15T11:07:58.209289Z",
     "iopub.status.idle": "2022-03-15T11:07:58.238019Z",
     "shell.execute_reply": "2022-03-15T11:07:58.237122Z",
     "shell.execute_reply.started": "2022-03-15T11:07:58.21004Z"
    },
    "id": "NZD-8GJd5Qsa",
    "outputId": "6a1b94d4-3f02-4f62-e73a-efdfbc35159a"
   },
   "outputs": [],
   "source": [
    "df.sort_values(by=[\"text_len\"], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gRp_Gs-2yeJR"
   },
   "source": [
    "## Sentiment column encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7IWVeUAFyeJR"
   },
   "source": [
    "The target column will be encoded by ordinal encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T11:07:58.242675Z",
     "iopub.status.busy": "2022-03-15T11:07:58.242134Z",
     "iopub.status.idle": "2022-03-15T11:07:58.293084Z",
     "shell.execute_reply": "2022-03-15T11:07:58.292476Z",
     "shell.execute_reply.started": "2022-03-15T11:07:58.242627Z"
    },
    "id": "NIS_nyXBG416"
   },
   "outputs": [],
   "source": [
    "#df['sentiment'] = df['sentiment'].replace({'religion':0,'age':1,'ethnicity':2,'gender':3,'not_cyberbullying':4})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ELMGaiE-F7yn"
   },
   "source": [
    "## Train - Test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jrLwRbummpEq"
   },
   "source": [
    "Now we need to split the dataset into a train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T11:07:58.29448Z",
     "iopub.status.busy": "2022-03-15T11:07:58.29412Z",
     "iopub.status.idle": "2022-03-15T11:07:58.298913Z",
     "shell.execute_reply": "2022-03-15T11:07:58.298124Z",
     "shell.execute_reply.started": "2022-03-15T11:07:58.294447Z"
    },
    "id": "BF01CgBtBONZ"
   },
   "outputs": [],
   "source": [
    "X = df['text_clean']\n",
    "y = df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g-tcinIJyeJS",
    "outputId": "47ab7d37-332d-474c-c691-06f2fca723ec"
   },
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4ib2TAabyeJT",
    "outputId": "1e20fbb9-f459-435b-89ca-9755e1518d32"
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7_74fv72yeJT"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "y_encoded = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T11:07:58.300839Z",
     "iopub.status.busy": "2022-03-15T11:07:58.300572Z",
     "iopub.status.idle": "2022-03-15T11:07:58.345104Z",
     "shell.execute_reply": "2022-03-15T11:07:58.344434Z",
     "shell.execute_reply.started": "2022-03-15T11:07:58.300806Z"
    },
    "id": "fvQexohPGAZf"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, stratify=y, random_state=seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A0d0lSX0GbNS"
   },
   "source": [
    "## Train - Validation split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0OsrkfnCmtk0"
   },
   "source": [
    "Moreover, we will further split the training set to extract a validation set, which will be used to monior the accuracy and loss to avoid overfitting during the model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T11:07:58.349926Z",
     "iopub.status.busy": "2022-03-15T11:07:58.345999Z",
     "iopub.status.idle": "2022-03-15T11:07:58.38522Z",
     "shell.execute_reply": "2022-03-15T11:07:58.384567Z",
     "shell.execute_reply.started": "2022-03-15T11:07:58.34989Z"
    },
    "id": "1KyTh6H5GbR4"
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, stratify=y_train, random_state=seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-03-15T11:07:58.387038Z",
     "iopub.status.busy": "2022-03-15T11:07:58.386848Z",
     "iopub.status.idle": "2022-03-15T11:07:58.393696Z",
     "shell.execute_reply": "2022-03-15T11:07:58.392977Z",
     "shell.execute_reply.started": "2022-03-15T11:07:58.387014Z"
    },
    "id": "N-mC3qyuBONc",
    "outputId": "049a2c62-9da3-4ecc-f5e1-fc82b2ba0da8"
   },
   "outputs": [],
   "source": [
    "(unique, counts) = np.unique(y_train, return_counts=True)\n",
    "np.asarray((unique, counts)).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "45AGAxHSnCXW"
   },
   "source": [
    "The classes are unbalanced, so it could be a good idea to oversample the training set such that all classes have the same count as the most populated one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Geqa3e3zGUNL"
   },
   "source": [
    "# Oversampling of training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T11:07:58.395835Z",
     "iopub.status.busy": "2022-03-15T11:07:58.394978Z",
     "iopub.status.idle": "2022-03-15T11:07:58.452354Z",
     "shell.execute_reply": "2022-03-15T11:07:58.45175Z",
     "shell.execute_reply.started": "2022-03-15T11:07:58.3958Z"
    },
    "id": "JN5OCli8BONe"
   },
   "outputs": [],
   "source": [
    "ros = RandomOverSampler()\n",
    "X_train, y_train = ros.fit_resample(np.array(X_train).reshape(-1, 1), np.array(y_train).reshape(-1, 1));\n",
    "train_os = pd.DataFrame(list(zip([x[0] for x in X_train], y_train)), columns = ['text_clean', 'sentiment']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T11:07:58.45381Z",
     "iopub.status.busy": "2022-03-15T11:07:58.453423Z",
     "iopub.status.idle": "2022-03-15T11:07:58.458737Z",
     "shell.execute_reply": "2022-03-15T11:07:58.458027Z",
     "shell.execute_reply.started": "2022-03-15T11:07:58.453773Z"
    },
    "id": "qAwnlrGoBONe"
   },
   "outputs": [],
   "source": [
    "X_train = train_os['text_clean'].values\n",
    "y_train = train_os['sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-03-15T11:07:58.460557Z",
     "iopub.status.busy": "2022-03-15T11:07:58.459844Z",
     "iopub.status.idle": "2022-03-15T11:07:58.470074Z",
     "shell.execute_reply": "2022-03-15T11:07:58.469327Z",
     "shell.execute_reply.started": "2022-03-15T11:07:58.460518Z"
    },
    "id": "zNopbN4gBONe",
    "outputId": "416f15de-15f2-4244-f309-cf0ea4dd53b8"
   },
   "outputs": [],
   "source": [
    "(unique, counts) = np.unique(y_train, return_counts=True)\n",
    "np.asarray((unique, counts)).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JqSWS6RDyeJX"
   },
   "source": [
    "# Naive Bayes baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "et43Rw8tyeJX"
   },
   "source": [
    "The first algorithm we will implement is Naive Bayes, which will be used as a simple baseline model. In order to use this algorithm, we need first need to preprocess the text data. <br>\n",
    "First, we will create a bag of words using CountVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T11:07:58.477077Z",
     "iopub.status.busy": "2022-03-15T11:07:58.476872Z",
     "iopub.status.idle": "2022-03-15T11:07:59.175229Z",
     "shell.execute_reply": "2022-03-15T11:07:59.174469Z",
     "shell.execute_reply.started": "2022-03-15T11:07:58.477034Z"
    },
    "id": "lRCIXRz5BONe"
   },
   "outputs": [],
   "source": [
    "clf = CountVectorizer()\n",
    "X_train_cv =  clf.fit_transform(X_train)\n",
    "X_test_cv = clf.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-3jKE2GyeJY"
   },
   "source": [
    "Then we apply TF-IFD transformation to associate weigths to the different words based on their frequency (rarer words will be given more importance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T11:07:59.176944Z",
     "iopub.status.busy": "2022-03-15T11:07:59.176681Z",
     "iopub.status.idle": "2022-03-15T11:07:59.210494Z",
     "shell.execute_reply": "2022-03-15T11:07:59.209819Z",
     "shell.execute_reply.started": "2022-03-15T11:07:59.17691Z"
    },
    "id": "B_FoQa2cBONf"
   },
   "outputs": [],
   "source": [
    "tf_transformer = TfidfTransformer(use_idf=True).fit(X_train_cv)\n",
    "X_train_tf = tf_transformer.transform(X_train_cv)\n",
    "X_test_tf = tf_transformer.transform(X_test_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w5Lc81d-yeJY"
   },
   "source": [
    "Finally we can instantiate the Naive Bayes model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T11:07:59.212133Z",
     "iopub.status.busy": "2022-03-15T11:07:59.211832Z",
     "iopub.status.idle": "2022-03-15T11:07:59.21588Z",
     "shell.execute_reply": "2022-03-15T11:07:59.215132Z",
     "shell.execute_reply.started": "2022-03-15T11:07:59.212097Z"
    },
    "id": "ecHNDL60BONf"
   },
   "outputs": [],
   "source": [
    "nb_clf = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "execution": {
     "iopub.execute_input": "2022-03-15T11:07:59.217563Z",
     "iopub.status.busy": "2022-03-15T11:07:59.217152Z",
     "iopub.status.idle": "2022-03-15T11:07:59.239617Z",
     "shell.execute_reply": "2022-03-15T11:07:59.238944Z",
     "shell.execute_reply.started": "2022-03-15T11:07:59.217525Z"
    },
    "id": "gj-9CqO9BONf",
    "outputId": "6ce62d2a-7aa9-450e-a052-c07b8693b294"
   },
   "outputs": [],
   "source": [
    "nb_clf.fit(X_train_tf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T11:07:59.240967Z",
     "iopub.status.busy": "2022-03-15T11:07:59.240705Z",
     "iopub.status.idle": "2022-03-15T11:07:59.247992Z",
     "shell.execute_reply": "2022-03-15T11:07:59.247196Z",
     "shell.execute_reply.started": "2022-03-15T11:07:59.24093Z"
    },
    "id": "Evr5z6nhBONf"
   },
   "outputs": [],
   "source": [
    "nb_pred = nb_clf.predict(X_test_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-03-15T11:07:59.249936Z",
     "iopub.status.busy": "2022-03-15T11:07:59.249429Z",
     "iopub.status.idle": "2022-03-15T11:07:59.271304Z",
     "shell.execute_reply": "2022-03-15T11:07:59.270643Z",
     "shell.execute_reply.started": "2022-03-15T11:07:59.249899Z"
    },
    "id": "4pUzmmo6BONf",
    "outputId": "f2f3e26c-fa51-4f2c-f7dc-904f78bcc547"
   },
   "outputs": [],
   "source": [
    "print('Classification Report for Naive Bayes:\\n',classification_report(y_test, nb_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-03-15T11:07:59.273094Z",
     "iopub.status.busy": "2022-03-15T11:07:59.272606Z",
     "iopub.status.idle": "2022-03-15T11:07:59.569002Z",
     "shell.execute_reply": "2022-03-15T11:07:59.56833Z",
     "shell.execute_reply.started": "2022-03-15T11:07:59.273042Z"
    },
    "id": "pC-HiqVNyeJa",
    "outputId": "49348a8c-a164-4acc-baa5-6b52fb44021f"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "cm=confusion_matrix(y_test,nb_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 503
    },
    "id": "0WzmTKBOcvT-",
    "outputId": "bee67156-9ff9-49f9-b9ca-a0afcea6c93c"
   },
   "outputs": [],
   "source": [
    "cm_display=ConfusionMatrixDisplay(cm)\n",
    "cm_display.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DnWIZmGlyeJb"
   },
   "source": [
    "**The performance scores of the algorithm is very good, with an overall accurcy of 85%.<br>\n",
    "We can observe how the predictions for the more populated classes have very high F1 scores (over 85%), while for the class \"non-cyberbullying\" the score is much lower.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TQEHbw5dyeJb"
   },
   "source": [
    "*Next we will implement a more complex algorithm to perform the classification, aiming to achieve higher accurcy than the baseline Naive Bayes model.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "itK4pUIiyeJb"
   },
   "source": [
    "# PyTorch Bi-LSTM RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xDS4MUCMyeJb"
   },
   "source": [
    "In this section, we will define a custom Bidirectional LSTM using PyTorch in order to perform the Sentiment Analysis on the tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QZpta6PbBONf"
   },
   "source": [
    "# Data preprocessing for LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lSHOll06yeJc"
   },
   "source": [
    "Similarly to what we did with Naive Bayes, we need to preprocess the data: in particular we will tokenize the sentences with a custom defined function. <br>\n",
    "The sentences will be converted to lists of number with padding to the max number of words in a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T11:07:59.570212Z",
     "iopub.status.busy": "2022-03-15T11:07:59.569957Z",
     "iopub.status.idle": "2022-03-15T11:07:59.578722Z",
     "shell.execute_reply": "2022-03-15T11:07:59.578039Z",
     "shell.execute_reply.started": "2022-03-15T11:07:59.570178Z"
    },
    "id": "PAhPX58mpM0i"
   },
   "outputs": [],
   "source": [
    "def Tokenize(column, seq_len):\n",
    "    ##Create vocabulary of words from column\n",
    "    corpus = [word for text in column for word in text.split()]\n",
    "    count_words = Counter(corpus)\n",
    "    sorted_words = count_words.most_common()\n",
    "    vocab_to_int = {w:i+1 for i, (w,c) in enumerate(sorted_words)}\n",
    "\n",
    "    ##Tokenize the columns text using the vocabulary\n",
    "    text_int = []\n",
    "    for text in column:\n",
    "        r = [vocab_to_int[word] for word in text.split()]\n",
    "        text_int.append(r)\n",
    "    ##Add padding to tokens\n",
    "    features = np.zeros((len(text_int), seq_len), dtype = int)\n",
    "    for i, review in enumerate(text_int):\n",
    "        if len(review) <= seq_len:\n",
    "            zeros = list(np.zeros(seq_len - len(review)))\n",
    "            new = zeros + review\n",
    "        else:\n",
    "            new = review[: seq_len]\n",
    "        features[i, :] = np.array(new)\n",
    "\n",
    "    return sorted_words, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T11:07:59.580606Z",
     "iopub.status.busy": "2022-03-15T11:07:59.579831Z",
     "iopub.status.idle": "2022-03-15T11:08:02.467103Z",
     "shell.execute_reply": "2022-03-15T11:08:02.466369Z",
     "shell.execute_reply.started": "2022-03-15T11:07:59.580568Z"
    },
    "id": "Huovl-tkq9pr"
   },
   "outputs": [],
   "source": [
    "vocabulary, tokenized_column = Tokenize(df[\"text_clean\"], max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C8pkbnZUyeJd"
   },
   "source": [
    "We can check how each tweet has been tokenized with an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "execution": {
     "iopub.execute_input": "2022-03-15T11:08:02.468838Z",
     "iopub.status.busy": "2022-03-15T11:08:02.468564Z",
     "iopub.status.idle": "2022-03-15T11:08:02.474539Z",
     "shell.execute_reply": "2022-03-15T11:08:02.473818Z",
     "shell.execute_reply.started": "2022-03-15T11:08:02.468802Z"
    },
    "id": "YGnWzA-HyeJd",
    "outputId": "01dfcd15-85bc-4dd1-e1e8-68658872fe0b"
   },
   "outputs": [],
   "source": [
    "df[\"text_clean\"].iloc[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wLdlDn33yeJd"
   },
   "source": [
    "This sentence is transformed to the vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-03-15T11:08:02.476812Z",
     "iopub.status.busy": "2022-03-15T11:08:02.476195Z",
     "iopub.status.idle": "2022-03-15T11:08:02.486749Z",
     "shell.execute_reply": "2022-03-15T11:08:02.486005Z",
     "shell.execute_reply.started": "2022-03-15T11:08:02.476739Z"
    },
    "id": "XphrfS9_yeJd",
    "outputId": "b0e5becf-c159-459f-fa95-dd9559d5d047"
   },
   "outputs": [],
   "source": [
    "tokenized_column[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hl3x7hqkyeJe"
   },
   "source": [
    "Moreover we can also check the TOP 20 most common words by extracting them from the vocabulary python dictionary we created with the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T11:08:02.488261Z",
     "iopub.status.busy": "2022-03-15T11:08:02.487939Z",
     "iopub.status.idle": "2022-03-15T11:08:02.493559Z",
     "shell.execute_reply": "2022-03-15T11:08:02.492636Z",
     "shell.execute_reply.started": "2022-03-15T11:08:02.488224Z"
    },
    "id": "6XgjzQohyeJe"
   },
   "outputs": [],
   "source": [
    "keys = []\n",
    "values = []\n",
    "for key, value in vocabulary[:20]:\n",
    "    keys.append(key)\n",
    "    values.append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-03-15T11:08:02.495949Z",
     "iopub.status.busy": "2022-03-15T11:08:02.495207Z",
     "iopub.status.idle": "2022-03-15T11:08:03.004752Z",
     "shell.execute_reply": "2022-03-15T11:08:03.004092Z",
     "shell.execute_reply.started": "2022-03-15T11:08:02.495845Z"
    },
    "id": "q4DYdJHGyeJe"
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(15, 5))\n",
    "# ax = sns.barplot(keys, values, palette='mako')\n",
    "# plt.title('Top 20 most common words', size=25)\n",
    "# ax.bar_label(ax.containers[0])\n",
    "# plt.ylabel(\"Words count\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U8u8G_-xyeJf"
   },
   "source": [
    "# Word Embedding by Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bPwRZKBsyeJf"
   },
   "source": [
    "Next, we will create a word embedding matrix using the original text tweets and the pre trained model Word2vec.<br>\n",
    "First, we create a list of words from the X_train vector created before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T11:08:03.006458Z",
     "iopub.status.busy": "2022-03-15T11:08:03.006208Z",
     "iopub.status.idle": "2022-03-15T11:08:03.247308Z",
     "shell.execute_reply": "2022-03-15T11:08:03.246532Z",
     "shell.execute_reply.started": "2022-03-15T11:08:03.006423Z"
    },
    "id": "9McFguGHyeJf"
   },
   "outputs": [],
   "source": [
    "Word2vec_train_data = list(map(lambda x: x.split(), X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2_hcaDquyeJf"
   },
   "source": [
    "We set a dimension of the embedding words, which can be seen as the number of featurs of each transformed word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T11:08:03.251564Z",
     "iopub.status.busy": "2022-03-15T11:08:03.251211Z",
     "iopub.status.idle": "2022-03-15T11:08:03.255622Z",
     "shell.execute_reply": "2022-03-15T11:08:03.254925Z",
     "shell.execute_reply.started": "2022-03-15T11:08:03.251534Z"
    },
    "id": "VwgSGsAeyeJg"
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SaSfDQLvyeJg"
   },
   "source": [
    "Then we can instaniate the Word2Vec model by passing the training words and chosen embedding dimension to the imported Word2vec object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gawBnuXZyeJg"
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T11:08:03.257897Z",
     "iopub.status.busy": "2022-03-15T11:08:03.257094Z",
     "iopub.status.idle": "2022-03-15T11:08:07.647416Z",
     "shell.execute_reply": "2022-03-15T11:08:07.646679Z",
     "shell.execute_reply.started": "2022-03-15T11:08:03.257848Z"
    },
    "id": "82M_-iUuyeJg"
   },
   "outputs": [],
   "source": [
    "word2vec_model = Word2Vec(Word2vec_train_data, vector_size=EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BQfo65ZzyeJh"
   },
   "source": [
    "Before defining the embedding matrix, we also need to choose the max number of words. We will extract the number of words from the vocabulary python dictionary we created before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-03-15T11:08:07.648932Z",
     "iopub.status.busy": "2022-03-15T11:08:07.64869Z",
     "iopub.status.idle": "2022-03-15T11:08:07.655556Z",
     "shell.execute_reply": "2022-03-15T11:08:07.654725Z",
     "shell.execute_reply.started": "2022-03-15T11:08:07.6489Z"
    },
    "id": "snmw8uvPyeJh",
    "outputId": "e27d25ff-325b-4caa-897e-349238deb291"
   },
   "outputs": [],
   "source": [
    "print(f\"Vocabulary size: {len(vocabulary) + 1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T11:08:07.657827Z",
     "iopub.status.busy": "2022-03-15T11:08:07.657277Z",
     "iopub.status.idle": "2022-03-15T11:08:07.663898Z",
     "shell.execute_reply": "2022-03-15T11:08:07.66317Z",
     "shell.execute_reply.started": "2022-03-15T11:08:07.657785Z"
    },
    "id": "sLeyW30NyeJh"
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(vocabulary) + 1 #+1 for the padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "peLk_NClyeJh"
   },
   "source": [
    "Finally we can define the embedding matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-03-15T11:08:07.665847Z",
     "iopub.status.busy": "2022-03-15T11:08:07.665364Z",
     "iopub.status.idle": "2022-03-15T11:08:07.736147Z",
     "shell.execute_reply": "2022-03-15T11:08:07.735241Z",
     "shell.execute_reply.started": "2022-03-15T11:08:07.665809Z"
    },
    "id": "zTSKNBm6yeJi",
    "outputId": "d9fb31fc-fc99-4fb2-8f8c-29d7556213aa"
   },
   "outputs": [],
   "source": [
    "#define empty embedding matrix\n",
    "embedding_matrix = np.zeros((VOCAB_SIZE, EMBEDDING_DIM))\n",
    "    \n",
    "#fill the embedding matrix with the pre trained values from word2vec\n",
    "#    corresponding to word (string), token (number associated to the word)\n",
    "for word, token in vocabulary:\n",
    "    if word2vec_model.wv.__contains__(word):\n",
    "        embedding_matrix[token] = word2vec_model.wv.__getitem__(word)\n",
    "\n",
    "print(\"Embedding Matrix Shape:\", embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rpwFLdQfyeJi"
   },
   "source": [
    "## Train - Validation - Test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u3juH0ksyeJi"
   },
   "source": [
    "Now we will use the tokenized sentences to create a training, validation and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T11:08:07.737824Z",
     "iopub.status.busy": "2022-03-15T11:08:07.737394Z",
     "iopub.status.idle": "2022-03-15T11:08:07.742222Z",
     "shell.execute_reply": "2022-03-15T11:08:07.741257Z",
     "shell.execute_reply.started": "2022-03-15T11:08:07.737777Z"
    },
    "id": "xPzbx9jFBONg"
   },
   "outputs": [],
   "source": [
    "X = tokenized_column\n",
    "y = df['sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T11:08:07.744333Z",
     "iopub.status.busy": "2022-03-15T11:08:07.743925Z",
     "iopub.status.idle": "2022-03-15T11:08:07.792207Z",
     "shell.execute_reply": "2022-03-15T11:08:07.791428Z",
     "shell.execute_reply.started": "2022-03-15T11:08:07.744297Z"
    },
    "id": "HQJ-XSMzBONg"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T11:08:07.794112Z",
     "iopub.status.busy": "2022-03-15T11:08:07.793596Z",
     "iopub.status.idle": "2022-03-15T11:08:07.826597Z",
     "shell.execute_reply": "2022-03-15T11:08:07.825822Z",
     "shell.execute_reply.started": "2022-03-15T11:08:07.794073Z"
    },
    "id": "aeca_zUVBONg"
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, stratify=y_train, random_state=seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BcaCQzVTyeJj"
   },
   "source": [
    "We can check the balance of the target classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-03-15T11:08:07.828572Z",
     "iopub.status.busy": "2022-03-15T11:08:07.828019Z",
     "iopub.status.idle": "2022-03-15T11:08:07.837767Z",
     "shell.execute_reply": "2022-03-15T11:08:07.836828Z",
     "shell.execute_reply.started": "2022-03-15T11:08:07.828532Z"
    },
    "id": "FX77tlMuBONg",
    "outputId": "c92858e0-0783-4b8b-83dc-07f2799c7b78"
   },
   "outputs": [],
   "source": [
    "(unique, counts) = np.unique(y_train, return_counts=True)\n",
    "np.asarray((unique, counts)).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4FjU22eEyeJk"
   },
   "source": [
    "And then apply random oversampling on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T11:08:07.842178Z",
     "iopub.status.busy": "2022-03-15T11:08:07.838859Z",
     "iopub.status.idle": "2022-03-15T11:08:07.872313Z",
     "shell.execute_reply": "2022-03-15T11:08:07.871562Z",
     "shell.execute_reply.started": "2022-03-15T11:08:07.842149Z"
    },
    "id": "sFRMuSdrBONh"
   },
   "outputs": [],
   "source": [
    "ros = RandomOverSampler()\n",
    "X_train_os, y_train_os = ros.fit_resample(np.array(X_train),np.array(y_train));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-03-15T11:08:07.874464Z",
     "iopub.status.busy": "2022-03-15T11:08:07.873688Z",
     "iopub.status.idle": "2022-03-15T11:08:07.883745Z",
     "shell.execute_reply": "2022-03-15T11:08:07.882989Z",
     "shell.execute_reply.started": "2022-03-15T11:08:07.874424Z"
    },
    "id": "OQl1UY2BUt-m",
    "outputId": "f982ad0b-a379-4393-ba89-055ad74ce686"
   },
   "outputs": [],
   "source": [
    "(unique, counts) = np.unique(y_train_os, return_counts=True)\n",
    "np.asarray((unique, counts)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JoXBmR9ryeJl",
    "outputId": "4d1ad622-998e-4529-d07d-80e288e9fe22"
   },
   "outputs": [],
   "source": [
    "y_train_os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "es0GLjC45vJ1",
    "outputId": "3e8c2509-dd8d-44a4-e0d9-80ea7dcea393"
   },
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mAnGYyXZ57Mc",
    "outputId": "951bf8ff-6dab-4bb5-9efa-70d06a21c171"
   },
   "outputs": [],
   "source": [
    "y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eeC5nIKf5Agm"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "y_train_os = le.fit_transform(y_train_os)\n",
    "y_test=le.fit_transform(y_test)\n",
    "y_valid=le.fit_transform(y_valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P4P0eTTuyeJl"
   },
   "outputs": [],
   "source": [
    "X_train_os = X_train_os.astype('float32')\n",
    "y_train_os=y_train_os.astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bbycjRWHyeJl"
   },
   "source": [
    "## PyTorch datasets and dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UfRC6sohyeJm"
   },
   "source": [
    "The three sets will be transformed to tensor datasets and dataloaders so we can extract the data in batches for the LSTM training, validation and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T11:08:07.885589Z",
     "iopub.status.busy": "2022-03-15T11:08:07.885213Z",
     "iopub.status.idle": "2022-03-15T11:08:07.890747Z",
     "shell.execute_reply": "2022-03-15T11:08:07.889819Z",
     "shell.execute_reply.started": "2022-03-15T11:08:07.885549Z"
    },
    "id": "O-e9FHZp5Qsc"
   },
   "outputs": [],
   "source": [
    "train_data = TensorDataset(torch.from_numpy(X_train_os),torch.from_numpy(y_train_os))\n",
    "test_data = TensorDataset(torch.from_numpy(X_test),torch.from_numpy(y_test))\n",
    "valid_data = TensorDataset(torch.from_numpy(X_valid),torch.from_numpy(y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T11:08:07.892855Z",
     "iopub.status.busy": "2022-03-15T11:08:07.892259Z",
     "iopub.status.idle": "2022-03-15T11:08:07.899739Z",
     "shell.execute_reply": "2022-03-15T11:08:07.898995Z",
     "shell.execute_reply.started": "2022-03-15T11:08:07.892817Z"
    },
    "id": "RfzfdlUFX8al"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T11:08:07.901599Z",
     "iopub.status.busy": "2022-03-15T11:08:07.9011Z",
     "iopub.status.idle": "2022-03-15T11:08:07.908456Z",
     "shell.execute_reply": "2022-03-15T11:08:07.907631Z",
     "shell.execute_reply.started": "2022-03-15T11:08:07.901562Z"
    },
    "id": "S2TGQjB4X7UQ"
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=BATCH_SIZE, drop_last=True) \n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=BATCH_SIZE, drop_last=True)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=BATCH_SIZE, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yezF0SdiyeJn"
   },
   "source": [
    "# PyTorch LSTM modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bh8mLXqMyeJn"
   },
   "source": [
    "Finally we can start the LSTM modeling. We start by setting some hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T11:08:07.910151Z",
     "iopub.status.busy": "2022-03-15T11:08:07.909762Z",
     "iopub.status.idle": "2022-03-15T11:08:07.94027Z",
     "shell.execute_reply": "2022-03-15T11:08:07.939553Z",
     "shell.execute_reply.started": "2022-03-15T11:08:07.910116Z"
    },
    "id": "dfLClXUs5Qsc"
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES = 5 #We are dealing with a multiclass classification of 5 classes\n",
    "HIDDEN_DIM = 100 #number of neurons of the internal state (internal neural network in the LSTM)\n",
    "LSTM_LAYERS = 1 #Number of stacked LSTM layers\n",
    "\n",
    "LR = 3e-4 #Learning rate\n",
    "DROPOUT = 0.5 #LSTM Dropout\n",
    "BIDIRECTIONAL = True #Boolean value to choose if to use a bidirectional LSTM or not\n",
    "EPOCHS = 5 #Number of training epoch\n",
    "\n",
    "DEVICE = 'cpu' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T11:08:07.941965Z",
     "iopub.status.busy": "2022-03-15T11:08:07.941664Z",
     "iopub.status.idle": "2022-03-15T11:08:07.954826Z",
     "shell.execute_reply": "2022-03-15T11:08:07.9541Z",
     "shell.execute_reply.started": "2022-03-15T11:08:07.941928Z"
    },
    "id": "InaVLcNV5Qsc"
   },
   "outputs": [],
   "source": [
    "class BiLSTM_Sentiment_Classifier(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes, lstm_layers, bidirectional,batch_size, dropout):\n",
    "        super(BiLSTM_Sentiment_Classifier,self).__init__()\n",
    "        \n",
    "        self.lstm_layers = lstm_layers\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_dim,\n",
    "                            hidden_dim,\n",
    "                            num_layers=lstm_layers,\n",
    "                            dropout=dropout,\n",
    "                            bidirectional=bidirectional,\n",
    "                            batch_first=True)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim*self.num_directions, num_classes)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        self.batch_size = x.size(0)\n",
    "        ##EMBEDDING LAYER\n",
    "        embedded = self.embedding(x)\n",
    "        #LSTM LAYERS\n",
    "        out, hidden = self.lstm(embedded, hidden)\n",
    "        #Extract only the hidden state from the last LSTM cell\n",
    "        out = out[:,-1,:]\n",
    "        #FULLY CONNECTED LAYERS\n",
    "        out = self.fc(out)\n",
    "        out = self.softmax(out)\n",
    "\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        #Initialization of the LSTM hidden and cell states\n",
    "        h0 = torch.zeros((self.lstm_layers*self.num_directions, batch_size, self.hidden_dim)).detach().to(DEVICE)\n",
    "        c0 = torch.zeros((self.lstm_layers*self.num_directions, batch_size, self.hidden_dim)).detach().to(DEVICE)\n",
    "        hidden = (h0, c0)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-03-15T11:08:07.959112Z",
     "iopub.status.busy": "2022-03-15T11:08:07.957483Z",
     "iopub.status.idle": "2022-03-15T11:08:10.622031Z",
     "shell.execute_reply": "2022-03-15T11:08:10.621136Z",
     "shell.execute_reply.started": "2022-03-15T11:08:07.959074Z"
    },
    "id": "n3Zs0lfW5Qsd",
    "outputId": "1bfb8273-300e-48d3-e8e4-367a89d86882"
   },
   "outputs": [],
   "source": [
    "model = BiLSTM_Sentiment_Classifier(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM,NUM_CLASSES, LSTM_LAYERS,BIDIRECTIONAL, BATCH_SIZE, DROPOUT)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "#Initialize embedding with the previously defined embedding matrix\n",
    "model.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
    "#Allow the embedding matrix to be fined tuned to better adapt to out dataset and get higher accuracy\n",
    "model.embedding.weight.requires_grad=True\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T11:08:10.623898Z",
     "iopub.status.busy": "2022-03-15T11:08:10.623472Z",
     "iopub.status.idle": "2022-03-15T11:08:10.629241Z",
     "shell.execute_reply": "2022-03-15T11:08:10.628467Z",
     "shell.execute_reply.started": "2022-03-15T11:08:10.623858Z"
    },
    "id": "ftkcRBFCY-Zp"
   },
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay = 5e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D06MTCUTyeJo"
   },
   "source": [
    "# LSTM Training loop "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r_v5J12PyeJp"
   },
   "source": [
    "Now we will define a custom training loop, where we include an early stopping functionality, and save only the best models in terms of validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-03-15T11:08:10.631087Z",
     "iopub.status.busy": "2022-03-15T11:08:10.630803Z",
     "iopub.status.idle": "2022-03-15T11:08:48.783473Z",
     "shell.execute_reply": "2022-03-15T11:08:48.78267Z",
     "shell.execute_reply.started": "2022-03-15T11:08:10.631037Z"
    },
    "id": "5rK3YPVT5Qsd",
    "outputId": "e17a40dc-86a2-4dfa-a505-a8b0948c836e"
   },
   "outputs": [],
   "source": [
    "total_step = len(train_loader)\n",
    "total_step_val = len(valid_loader)\n",
    "\n",
    "early_stopping_patience = 4\n",
    "early_stopping_counter = 0\n",
    "\n",
    "valid_acc_max = 0 # Initialize best accuracy top 0\n",
    "\n",
    "for e in range(EPOCHS):\n",
    "\n",
    "    #lists to host the train and validation losses of every batch for each epoch\n",
    "    train_loss, valid_loss  = [], []\n",
    "    #lists to host the train and validation accuracy of every batch for each epoch\n",
    "    train_acc, valid_acc  = [], []\n",
    "\n",
    "    #lists to host the train and validation predictions of every batch for each epoch\n",
    "    y_train_list, y_val_list = [], []\n",
    "\n",
    "    #initalize number of total and correctly classified texts during training and validation\n",
    "    correct, correct_val = 0, 0\n",
    "    total, total_val = 0, 0\n",
    "    running_loss, running_loss_val = 0, 0\n",
    "\n",
    "\n",
    "    ####TRAINING LOOP####\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE) #load features and targets in device\n",
    "\n",
    "        h = model.init_hidden(labels.size(0))\n",
    "\n",
    "        model.zero_grad() #reset gradients \n",
    "\n",
    "        inputs=inputs.long()\n",
    "\n",
    "        output, h = model(inputs,h) #get output and hidden states from LSTM network\n",
    "        \n",
    "        #loss = criterion(output, labels)\n",
    "        loss = criterion(output, labels.long())\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        y_pred_train = torch.argmax(output, dim=1) #get tensor of predicted values on the training set\n",
    "        y_train_list.extend(y_pred_train.squeeze().tolist()) #transform tensor to list and the values to the list\n",
    "        \n",
    "        correct += torch.sum(y_pred_train==labels).item() #count correctly classified texts per batch\n",
    "        total += labels.size(0) #count total texts per batch\n",
    "\n",
    "    train_loss.append(running_loss / total_step)\n",
    "    train_acc.append(100 * correct / total)\n",
    "\n",
    "    ####VALIDATION LOOP####\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        for inputs, labels in valid_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "            val_h = model.init_hidden(labels.size(0))\n",
    "\n",
    "            output, val_h = model(inputs, val_h)\n",
    "\n",
    "            val_loss = criterion(output, labels)\n",
    "            running_loss_val += val_loss.item()\n",
    "\n",
    "            y_pred_val = torch.argmax(output, dim=1)\n",
    "            y_val_list.extend(y_pred_val.squeeze().tolist())\n",
    "\n",
    "            correct_val += torch.sum(y_pred_val==labels).item()\n",
    "            total_val += labels.size(0)\n",
    "\n",
    "        valid_loss.append(running_loss_val / total_step_val)\n",
    "        valid_acc.append(100 * correct_val / total_val)\n",
    "\n",
    "    #Save model if validation accuracy increases\n",
    "    if np.mean(valid_acc) >= valid_acc_max:\n",
    "        torch.save(model.state_dict(), './state_dict.pt')\n",
    "        print(f'Epoch {e+1}:Validation accuracy increased ({valid_acc_max:.6f} --> {np.mean(valid_acc):.6f}).  Saving model ...')\n",
    "        valid_acc_max = np.mean(valid_acc)\n",
    "        early_stopping_counter=0 #reset counter if validation accuracy increases\n",
    "    else:\n",
    "        print(f'Epoch {e+1}:Validation accuracy did not increase')\n",
    "        early_stopping_counter+=1 #increase counter if validation accuracy does not increase\n",
    "        \n",
    "    if early_stopping_counter > early_stopping_patience:\n",
    "        print('Early stopped at epoch :', e+1)\n",
    "        break\n",
    "    \n",
    "    print(f'\\tTrain_loss : {np.mean(train_loss):.4f} Val_loss : {np.mean(valid_loss):.4f}')\n",
    "    print(f'\\tTrain_acc : {np.mean(train_acc):.3f}% Val_acc : {np.mean(valid_acc):.3f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-03-15T11:08:48.785121Z",
     "iopub.status.busy": "2022-03-15T11:08:48.784648Z",
     "iopub.status.idle": "2022-03-15T11:08:48.815436Z",
     "shell.execute_reply": "2022-03-15T11:08:48.814716Z",
     "shell.execute_reply.started": "2022-03-15T11:08:48.785075Z"
    },
    "id": "wfTLdAi45Qsd",
    "outputId": "61628c72-f6df-46d8-c4dc-bdecb6fd403e"
   },
   "outputs": [],
   "source": [
    "# Loading the best model\n",
    "model.load_state_dict(torch.load('./state_dict.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JqmVEMgSyeJq"
   },
   "source": [
    "# LSTM Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T11:08:48.817045Z",
     "iopub.status.busy": "2022-03-15T11:08:48.816743Z",
     "iopub.status.idle": "2022-03-15T11:08:49.439464Z",
     "shell.execute_reply": "2022-03-15T11:08:49.438724Z",
     "shell.execute_reply.started": "2022-03-15T11:08:48.817009Z"
    },
    "id": "3ERsOjzW41tF"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "y_pred_list = []\n",
    "y_test_list = []\n",
    "for inputs, labels in test_loader:\n",
    "    inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "    test_h = model.init_hidden(labels.size(0))\n",
    "\n",
    "    output, val_h = model(inputs, test_h)\n",
    "    y_pred_test = torch.argmax(output, dim=1)\n",
    "    y_pred_list.extend(y_pred_test.squeeze().tolist())\n",
    "    y_test_list.extend(labels.squeeze().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-03-15T11:08:49.440979Z",
     "iopub.status.busy": "2022-03-15T11:08:49.440727Z",
     "iopub.status.idle": "2022-03-15T11:08:49.469431Z",
     "shell.execute_reply": "2022-03-15T11:08:49.468722Z",
     "shell.execute_reply.started": "2022-03-15T11:08:49.440945Z"
    },
    "id": "tu6ex4zC5Qse",
    "outputId": "b3507ab7-de92-4df3-d9ba-ba34f4e2ff07"
   },
   "outputs": [],
   "source": [
    "print('Classification Report for Bi-LSTM :\\n', classification_report(y_test_list, y_pred_list, target_names=sentiments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-03-15T11:08:49.471249Z",
     "iopub.status.busy": "2022-03-15T11:08:49.470591Z",
     "iopub.status.idle": "2022-03-15T11:08:49.77764Z",
     "shell.execute_reply": "2022-03-15T11:08:49.776925Z",
     "shell.execute_reply.started": "2022-03-15T11:08:49.471212Z"
    },
    "id": "vmE9K77s1qv_",
    "outputId": "a8591e23-4576-4f97-95e0-935780919397"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "cm=confusion_matrix(y_test_list,y_pred_list)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 503
    },
    "id": "8YSR3htlf0DM",
    "outputId": "3478cdf4-fcc4-4d1d-e132-a4edbf18e30d"
   },
   "outputs": [],
   "source": [
    "cm_display=ConfusionMatrixDisplay(cm)\n",
    "cm_display.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "68WPk3iRyeJr"
   },
   "source": [
    "**The performance scores of the algorithm are very high, with an overall accuracy of 94%.**<br>\n",
    "**In particular, the F1 scores for the more populated classes are over 95%**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ctadxhSRyeJs"
   },
   "source": [
    "# BERT Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9D0GSZioyeJs"
   },
   "source": [
    "In this section, we will load a pre trained BERT model from the Hugging Face library and fine tune it for our classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dlGJmd5SyeJs"
   },
   "source": [
    "First, we need to split the dataset into train - validation - test again since we need to tokenize the sentences differently from before (Naive Bayes and LSTM)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u3tThyoHyeJs"
   },
   "source": [
    "## Train - Validation - Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T11:08:49.779324Z",
     "iopub.status.busy": "2022-03-15T11:08:49.779036Z",
     "iopub.status.idle": "2022-03-15T11:08:49.78352Z",
     "shell.execute_reply": "2022-03-15T11:08:49.782674Z",
     "shell.execute_reply.started": "2022-03-15T11:08:49.779287Z"
    },
    "id": "OYd1Z0f7yeJt"
   },
   "outputs": [],
   "source": [
    "X = df['text_clean'].values\n",
    "y = df['sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T11:08:49.78529Z",
     "iopub.status.busy": "2022-03-15T11:08:49.785022Z",
     "iopub.status.idle": "2022-03-15T11:08:49.81908Z",
     "shell.execute_reply": "2022-03-15T11:08:49.818455Z",
     "shell.execute_reply.started": "2022-03-15T11:08:49.785256Z"
    },
    "id": "-2PCUf-XyeJt"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T11:08:49.820462Z",
     "iopub.status.busy": "2022-03-15T11:08:49.820217Z",
     "iopub.status.idle": "2022-03-15T11:08:49.846298Z",
     "shell.execute_reply": "2022-03-15T11:08:49.845668Z",
     "shell.execute_reply.started": "2022-03-15T11:08:49.820427Z"
    },
    "id": "BWIyVVoTyeJt"
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, stratify=y_train, random_state=seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qCTCrMOcyeJt"
   },
   "source": [
    "As seen before, we oversample the text to the majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T11:08:49.847775Z",
     "iopub.status.busy": "2022-03-15T11:08:49.847504Z",
     "iopub.status.idle": "2022-03-15T11:08:49.864409Z",
     "shell.execute_reply": "2022-03-15T11:08:49.863773Z",
     "shell.execute_reply.started": "2022-03-15T11:08:49.847739Z"
    },
    "id": "K1Mp-rk0yeJu"
   },
   "outputs": [],
   "source": [
    "ros = RandomOverSampler()\n",
    "X_train_os, y_train_os = ros.fit_resample(np.array(X_train).reshape(-1,1),np.array(y_train).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T11:08:49.865865Z",
     "iopub.status.busy": "2022-03-15T11:08:49.865601Z",
     "iopub.status.idle": "2022-03-15T11:08:49.870838Z",
     "shell.execute_reply": "2022-03-15T11:08:49.870171Z",
     "shell.execute_reply.started": "2022-03-15T11:08:49.865829Z"
    },
    "id": "l5dyRn30yeJu"
   },
   "outputs": [],
   "source": [
    "X_train_os = X_train_os.flatten()\n",
    "y_train_os = y_train_os.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-03-15T11:08:49.872414Z",
     "iopub.status.busy": "2022-03-15T11:08:49.872003Z",
     "iopub.status.idle": "2022-03-15T11:08:49.882321Z",
     "shell.execute_reply": "2022-03-15T11:08:49.881433Z",
     "shell.execute_reply.started": "2022-03-15T11:08:49.872376Z"
    },
    "id": "w24u5xZYyeJv",
    "outputId": "54e6a00d-bccb-432a-9a9d-959a59108e4b"
   },
   "outputs": [],
   "source": [
    "(unique, counts) = np.unique(y_train_os, return_counts=True)\n",
    "np.asarray((unique, counts)).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ne5oFO9FyeJv"
   },
   "source": [
    "# BERT Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uqix-gpTyeJv"
   },
   "source": [
    "Since we need to tokenize the tweets (get \"input ids\" and \"attention masks\") for BERT, we load the specific BERT tokenizer from the Hugging Face library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "d8e556ad664a467ca4db1568fd192be6",
      "20e7eee8c01d435eab7e076732ac9bab",
      "e67e18c7d4644fe3b7cf38451ed27377",
      "ac46cacd630d410c8dad740229b79469",
      "7b1446638cf543deaf6646f85c822ba2",
      "738205b4a64542a39c4c2520af16dab2",
      "e1f73c8646ee4c1086d211ce86ab003c",
      "e86b49099a464e79a02f56e2742ecbcb",
      "7bc393c41acb45cf8a3dd9dd3e51603f",
      "17e9251916ec454397d1afd1452e3ea9",
      "db53dc29a9414459ab2ad27cb963fc4f",
      "4ebc1c3817f54e019d5557ae19814936",
      "aa4897a5c3a747338f229135d54d13d6",
      "ff3f4749c9dc4686be0629e823f2547e",
      "31911d3ac499476ab6b34813a651bed4",
      "e12bb278a6d74fd59254200e6c26aca1",
      "f8da253dc8c34bc28361bc92b0e0cc61",
      "df17dcfef6834e96a8ecfd355cf10f4e",
      "5e2238c0f4a04f2fa6a3c20ae7e5646b",
      "7b415fe6f4b349d897d29634516d959f",
      "437058f1482c43c0b9b300cbc4ddf86b",
      "3c90ee094a4d4feb8bb99dd3b0f7d0fd",
      "4effba97b2474449aa904e432cb45a8d",
      "9c64220a271044fdb4f5b831ee4bb907",
      "95939e89e4654a75afdd6b9affae2848",
      "21ae65d1f376401c8f0f43e38eb9a6ab",
      "83ada81d295c442989b34f6421d6ee52",
      "40e35c80b02d4007bfb3e1159c17e78a",
      "85fcd76fc9774fb5b5626ad17bc58c0a",
      "4374234b677147fca74cc0d342088e46",
      "bed3482961a7486bb0fdd1abb2f250dd",
      "0a4eed466e3a4a3281b7bd56891962e1",
      "3be7df48f6c145278d3954f026944d08"
     ]
    },
    "execution": {
     "iopub.execute_input": "2022-03-15T11:08:49.883793Z",
     "iopub.status.busy": "2022-03-15T11:08:49.883488Z",
     "iopub.status.idle": "2022-03-15T11:08:53.135288Z",
     "shell.execute_reply": "2022-03-15T11:08:53.134544Z",
     "shell.execute_reply.started": "2022-03-15T11:08:49.883757Z"
    },
    "id": "ItCnlmt6yeJw",
    "outputId": "31d65eee-fcef-45e2-ce5d-a46e1decd02d"
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T08:35:46.046364Z",
     "iopub.status.busy": "2022-03-15T08:35:46.04601Z",
     "iopub.status.idle": "2022-03-15T08:35:46.076692Z",
     "shell.execute_reply": "2022-03-15T08:35:46.075417Z",
     "shell.execute_reply.started": "2022-03-15T08:35:46.046277Z"
    },
    "id": "2YmrviutyeJw"
   },
   "source": [
    "Then we define a custom tokenizer function using the loaded tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T11:08:53.136923Z",
     "iopub.status.busy": "2022-03-15T11:08:53.136672Z",
     "iopub.status.idle": "2022-03-15T11:08:53.14452Z",
     "shell.execute_reply": "2022-03-15T11:08:53.143696Z",
     "shell.execute_reply.started": "2022-03-15T11:08:53.136889Z"
    },
    "id": "GxgOwxU5yeJw"
   },
   "outputs": [],
   "source": [
    "def bert_tokenizer(data):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    for sent in data:\n",
    "        encoded_sent = tokenizer.encode_plus(\n",
    "            text=sent,\n",
    "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]` special tokens\n",
    "            max_length=MAX_LEN,             # Choose max length to truncate/pad\n",
    "            pad_to_max_length=True,         # Pad sentence to max length \n",
    "            return_attention_mask=True      # Return attention mask\n",
    "            )\n",
    "        input_ids.append(encoded_sent.get('input_ids'))\n",
    "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TBd7AVz4yeJw"
   },
   "source": [
    "Since we need to specify the length of the longest tokenized sentence, we tokenize the train tweets using the \"encode\" method of the original BERT tokenizer and check the longest sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-03-15T11:08:53.146471Z",
     "iopub.status.busy": "2022-03-15T11:08:53.146211Z",
     "iopub.status.idle": "2022-03-15T11:09:09.361351Z",
     "shell.execute_reply": "2022-03-15T11:09:09.360539Z",
     "shell.execute_reply.started": "2022-03-15T11:08:53.146436Z"
    },
    "id": "lamqmY1dyeJw",
    "outputId": "90419729-3873-48f7-d76d-4a34a2c74c80"
   },
   "outputs": [],
   "source": [
    "# Tokenize train tweets\n",
    "encoded_tweets = [tokenizer.encode(sent, add_special_tokens=True) for sent in X_train]\n",
    "\n",
    "# Find the longest tokenized tweet\n",
    "max_len = max([len(sent) for sent in encoded_tweets])\n",
    "print('Max length: ', max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "llkJwqfCyeJx"
   },
   "source": [
    "We can choose the max length as 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T11:09:09.363189Z",
     "iopub.status.busy": "2022-03-15T11:09:09.362699Z",
     "iopub.status.idle": "2022-03-15T11:09:09.367329Z",
     "shell.execute_reply": "2022-03-15T11:09:09.366498Z",
     "shell.execute_reply.started": "2022-03-15T11:09:09.363148Z"
    },
    "id": "3jmiqjJAyeJx"
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J9hDy3BnyeJx"
   },
   "source": [
    "Then we can tokenize the train, validation and test tweets using the custom define tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-03-15T11:09:09.371264Z",
     "iopub.status.busy": "2022-03-15T11:09:09.368784Z",
     "iopub.status.idle": "2022-03-15T11:09:35.494332Z",
     "shell.execute_reply": "2022-03-15T11:09:35.493469Z",
     "shell.execute_reply.started": "2022-03-15T11:09:09.371224Z"
    },
    "id": "_3qjEOlcyeJy",
    "outputId": "e8f41ff9-0be2-4f3f-c516-74ce9ac2d8cd"
   },
   "outputs": [],
   "source": [
    "train_inputs, train_masks = bert_tokenizer(X_train_os)\n",
    "val_inputs, val_masks = bert_tokenizer(X_valid)\n",
    "test_inputs, test_masks = bert_tokenizer(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-O7DoAqLyeJy"
   },
   "source": [
    "## Data preprocessing for PyTorch BERT model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IWz08wf5yeJy"
   },
   "source": [
    "Since we are using the BERT model built on PyTorch, we need to convert the arrays to pytorch tensors and create dataloaders for the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xa0Rmf6W-OTO",
    "outputId": "9d67b3d9-4e3c-4209-fa84-be96c8a3d68d"
   },
   "outputs": [],
   "source": [
    "y_train_os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D4abDONj-t7n"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "y_train_os = le.fit_transform(y_train_os)\n",
    "y_test=le.fit_transform(y_test)\n",
    "y_valid=le.fit_transform(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T11:09:35.495866Z",
     "iopub.status.busy": "2022-03-15T11:09:35.495601Z",
     "iopub.status.idle": "2022-03-15T11:09:35.50066Z",
     "shell.execute_reply": "2022-03-15T11:09:35.499956Z",
     "shell.execute_reply.started": "2022-03-15T11:09:35.495833Z"
    },
    "id": "QhNkXND0yeJz"
   },
   "outputs": [],
   "source": [
    "# Convert target columns to pytorch tensors format\n",
    "train_labels = torch.from_numpy(y_train_os)\n",
    "val_labels = torch.from_numpy(y_valid)\n",
    "test_labels = torch.from_numpy(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Jg6DCwVyeJz"
   },
   "source": [
    "## Dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5WejwbxCyeJz"
   },
   "source": [
    "To fine-tune the BERT model, the original authors recommend a batch size of 16 or 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T11:09:35.502562Z",
     "iopub.status.busy": "2022-03-15T11:09:35.502071Z",
     "iopub.status.idle": "2022-03-15T11:09:35.512198Z",
     "shell.execute_reply": "2022-03-15T11:09:35.511403Z",
     "shell.execute_reply.started": "2022-03-15T11:09:35.502526Z"
    },
    "id": "PU3h_psOyeJz"
   },
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T11:09:35.514893Z",
     "iopub.status.busy": "2022-03-15T11:09:35.514017Z",
     "iopub.status.idle": "2022-03-15T11:09:35.52287Z",
     "shell.execute_reply": "2022-03-15T11:09:35.522183Z",
     "shell.execute_reply.started": "2022-03-15T11:09:35.514852Z"
    },
    "id": "qAqbGakpyeJ0"
   },
   "outputs": [],
   "source": [
    "# Create the DataLoader for our training set\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for our validation set\n",
    "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for our test set\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gSjaIBAyyeJ0"
   },
   "source": [
    "# BERT Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoLu35ssyeJ0"
   },
   "source": [
    "Now we can create a custom BERT classifier class, including the original BERT model (made of transformer layers) and additional Dense layers to perform the desired classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-03-15T11:09:35.525Z",
     "iopub.status.busy": "2022-03-15T11:09:35.524455Z",
     "iopub.status.idle": "2022-03-15T11:09:35.539406Z",
     "shell.execute_reply": "2022-03-15T11:09:35.538603Z",
     "shell.execute_reply.started": "2022-03-15T11:09:35.524962Z"
    },
    "id": "K1HM685OyeJ0",
    "outputId": "154f0779-23bd-4425-9da9-34f8e68a83df"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "class Bert_Classifier(nn.Module):\n",
    "    def __init__(self, freeze_bert=False):\n",
    "        super(Bert_Classifier, self).__init__()\n",
    "        # Specify hidden size of BERT, hidden size of the classifier, and number of labels\n",
    "        n_input = 768\n",
    "        n_hidden = 50\n",
    "        n_output = 5\n",
    "        # Instantiate BERT model\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        # Add dense layers to perform the classification\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(n_input,  n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_output)\n",
    "        )\n",
    "        # Add possibility to freeze the BERT model\n",
    "        # to avoid fine tuning BERT params (usually leads to worse results)\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Feed input data to BERT\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask)\n",
    "        \n",
    "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
    "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
    "\n",
    "        # Feed input to classifier to compute logits\n",
    "        logits = self.classifier(last_hidden_state_cls)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8lDUyQrEyeJ1"
   },
   "source": [
    "Moreover, since we want to define a learning rate scheduler, we define a custom \"initalize_model\" function as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T11:09:35.542328Z",
     "iopub.status.busy": "2022-03-15T11:09:35.542094Z",
     "iopub.status.idle": "2022-03-15T11:09:35.552075Z",
     "shell.execute_reply": "2022-03-15T11:09:35.551276Z",
     "shell.execute_reply.started": "2022-03-15T11:09:35.542296Z"
    },
    "id": "GleWFxARyeJ1"
   },
   "outputs": [],
   "source": [
    "def initialize_model(epochs=4):\n",
    "    # Instantiate Bert Classifier\n",
    "    bert_classifier = Bert_Classifier(freeze_bert=False)\n",
    "    \n",
    "    bert_classifier.to(device)\n",
    "\n",
    "    # Set up optimizer\n",
    "    optimizer = AdamW(bert_classifier.parameters(),\n",
    "                      lr=5e-5,    # learning rate, set to default value\n",
    "                      eps=1e-8    # decay, set to default value\n",
    "                      )\n",
    "    \n",
    "    ### Set up learning rate scheduler ###\n",
    "\n",
    "    # Calculate total number of training steps\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "    # Defint the scheduler\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps=0, # Default value\n",
    "                                                num_training_steps=total_steps)\n",
    "    return bert_classifier, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tPe1rb5HyeJ1"
   },
   "source": [
    "We also specify the use of GPU if present (highly recommended for the fine tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T11:09:35.553826Z",
     "iopub.status.busy": "2022-03-15T11:09:35.553375Z",
     "iopub.status.idle": "2022-03-15T11:09:35.563926Z",
     "shell.execute_reply": "2022-03-15T11:09:35.563062Z",
     "shell.execute_reply.started": "2022-03-15T11:09:35.553786Z"
    },
    "id": "55PApt_syeJ2"
   },
   "outputs": [],
   "source": [
    "device = 'cpu' if torch.cuda.is_available() else 'cpu'\n",
    "EPOCHS=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "03Fr4GgMyeJ2"
   },
   "source": [
    "And then we intialize the BERT model calling the \"initialize_model\" function we defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156,
     "referenced_widgets": [
      "e755e5bc6b2b4933afca483a50911598",
      "7af8778142244a4f956813e0b9b7db71",
      "3b1f16cbcc4344cc93f0d222fa305cc5",
      "deefa33b61584260bd4b5bfbee1b9b1e",
      "d134fc4f02384d228bd998cf54917331",
      "20f59bc4ca3048a1baab3e77a45b62e8",
      "619135b67c814dcc8918e60ed9fc4ea5",
      "a95d58405a1d471dba15507f3fbd5398",
      "f06550b5dc3a498fa9a9f9a0e0fc21d3",
      "8ebd4fc54ad74089bb9c6d65598f6cf6",
      "d67636adc6d54b408874c07332589d93"
     ]
    },
    "execution": {
     "iopub.execute_input": "2022-03-15T11:09:35.565878Z",
     "iopub.status.busy": "2022-03-15T11:09:35.565057Z",
     "iopub.status.idle": "2022-03-15T11:09:38.401591Z",
     "shell.execute_reply": "2022-03-15T11:09:38.400718Z",
     "shell.execute_reply.started": "2022-03-15T11:09:35.565838Z"
    },
    "id": "LcGu0idkyeJ2",
    "outputId": "2322ca3c-6db5-412a-d691-2d1cba2015aa"
   },
   "outputs": [],
   "source": [
    "bert_classifier, optimizer, scheduler = initialize_model(epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fd0VvwUuyeJ2"
   },
   "source": [
    "# BERT Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tBSIV-9ryeJ2"
   },
   "source": [
    "After defining the custom BERT classifier model, we are ready to start the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T11:09:38.404266Z",
     "iopub.status.busy": "2022-03-15T11:09:38.403435Z",
     "iopub.status.idle": "2022-03-15T11:09:38.423282Z",
     "shell.execute_reply": "2022-03-15T11:09:38.42246Z",
     "shell.execute_reply.started": "2022-03-15T11:09:38.40422Z"
    },
    "id": "yJLBE0shyeJ3"
   },
   "outputs": [],
   "source": [
    "# Define Cross entropy Loss function for the multiclass classification task\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def bert_train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n",
    "\n",
    "    print(\"Start training...\\n\")\n",
    "    for epoch_i in range(epochs):\n",
    "        print(\"-\"*10)\n",
    "        print(\"Epoch : {}\".format(epoch_i+1))\n",
    "        print(\"-\"*10)\n",
    "        print(\"-\"*38)\n",
    "        print(f\"{'BATCH NO.':^7} | {'TRAIN LOSS':^12} | {'ELAPSED (s)':^9}\")\n",
    "        print(\"-\"*38)\n",
    "\n",
    "        # Measure the elapsed time of each epoch\n",
    "        t0_epoch, t0_batch = time.time(), time.time()\n",
    "\n",
    "        # Reset tracking variables at the beginning of each epoch\n",
    "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "        \n",
    "        ###TRAINING###\n",
    "\n",
    "        # Put the model into the training mode\n",
    "        model.train()\n",
    "\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            batch_counts +=1\n",
    "            \n",
    "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            # Zero out any previously calculated gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Perform a forward pass and get logits.\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "            # Compute loss and accumulate the loss values\n",
    "            loss = loss_fn(logits, b_labels)\n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Perform a backward pass to calculate gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # Update model parameters:\n",
    "            # fine tune BERT params and train additional dense layers\n",
    "            optimizer.step()\n",
    "            # update learning rate\n",
    "            scheduler.step()\n",
    "\n",
    "            # Print the loss values and time elapsed for every 100 batches\n",
    "            if (step % 100 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
    "                # Calculate time elapsed for 20 batches\n",
    "                time_elapsed = time.time() - t0_batch\n",
    "                \n",
    "                print(f\"{step:^9} | {batch_loss / batch_counts:^12.6f} | {time_elapsed:^9.2f}\")\n",
    "\n",
    "                # Reset batch tracking variables\n",
    "                batch_loss, batch_counts = 0, 0\n",
    "                t0_batch = time.time()\n",
    "\n",
    "        # Calculate the average loss over the entire training data\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        ###EVALUATION###\n",
    "        \n",
    "        # Put the model into the evaluation mode\n",
    "        model.eval()\n",
    "        \n",
    "        # Define empty lists to host accuracy and validation for each batch\n",
    "        val_accuracy = []\n",
    "        val_loss = []\n",
    "\n",
    "        for batch in val_dataloader:\n",
    "            batch_input_ids, batch_attention_mask, batch_labels = tuple(t.to(device) for t in batch)\n",
    "            \n",
    "            # We do not want to update the params during the evaluation,\n",
    "            # So we specify that we dont want to compute the gradients of the tensors\n",
    "            # by calling the torch.no_grad() method\n",
    "            with torch.no_grad():\n",
    "                logits = model(batch_input_ids, batch_attention_mask)\n",
    "\n",
    "            loss = loss_fn(logits, batch_labels)\n",
    "\n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "            # Get the predictions starting from the logits (get index of highest logit)\n",
    "            preds = torch.argmax(logits, dim=1).flatten()\n",
    "\n",
    "            # Calculate the validation accuracy \n",
    "            accuracy = (preds == batch_labels).cpu().numpy().mean() * 100\n",
    "            val_accuracy.append(accuracy)\n",
    "\n",
    "        # Compute the average accuracy and loss over the validation set\n",
    "        val_loss = np.mean(val_loss)\n",
    "        val_accuracy = np.mean(val_accuracy)\n",
    "        \n",
    "        # Print performance over the entire training data\n",
    "        time_elapsed = time.time() - t0_epoch\n",
    "        print(\"-\"*61)\n",
    "        print(f\"{'AVG TRAIN LOSS':^12} | {'VAL LOSS':^10} | {'VAL ACCURACY (%)':^9} | {'ELAPSED (s)':^9}\")\n",
    "        print(\"-\"*61)\n",
    "        print(f\"{avg_train_loss:^14.6f} | {val_loss:^10.6f} | {val_accuracy:^17.2f} | {time_elapsed:^9.2f}\")\n",
    "        print(\"-\"*61)\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 450
    },
    "execution": {
     "iopub.execute_input": "2022-03-15T11:09:38.425092Z",
     "iopub.status.busy": "2022-03-15T11:09:38.424368Z",
     "iopub.status.idle": "2022-03-15T11:21:27.729518Z",
     "shell.execute_reply": "2022-03-15T11:21:27.727972Z",
     "shell.execute_reply.started": "2022-03-15T11:09:38.42504Z"
    },
    "id": "YfbTpjytyeJ3",
    "outputId": "8a21ea84-54ef-4925-f5cb-6e71b4978b4f"
   },
   "outputs": [],
   "source": [
    "bert_train(bert_classifier, train_dataloader, val_dataloader, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gRhWyW-NyeJ4"
   },
   "source": [
    "# BERT Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u_5lurGDyeJ4"
   },
   "source": [
    "Now we define a function similar to the model \"evaluation\", where we feed to the model the test data instead of the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T11:21:27.730974Z",
     "iopub.status.busy": "2022-03-15T11:21:27.73073Z",
     "iopub.status.idle": "2022-03-15T11:21:27.738662Z",
     "shell.execute_reply": "2022-03-15T11:21:27.737958Z",
     "shell.execute_reply.started": "2022-03-15T11:21:27.730937Z"
    },
    "id": "qCXXGLy3yeJ5"
   },
   "outputs": [],
   "source": [
    "def bert_predict(model, test_dataloader):\n",
    "    \n",
    "    # Define empty list to host the predictions\n",
    "    preds_list = []\n",
    "    \n",
    "    # Put the model into evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    for batch in test_dataloader:\n",
    "        batch_input_ids, batch_attention_mask = tuple(t.to(device) for t in batch)[:2]\n",
    "        \n",
    "        # Avoid gradient calculation of tensors by using \"no_grad()\" method\n",
    "        with torch.no_grad():\n",
    "            logit = model(batch_input_ids, batch_attention_mask)\n",
    "        \n",
    "        # Get index of highest logit\n",
    "        pred = torch.argmax(logit,dim=1).cpu().numpy()\n",
    "        # Append predicted class to list\n",
    "        preds_list.extend(pred)\n",
    "\n",
    "    return preds_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JzjHnOStyeJ5"
   },
   "source": [
    "Then we can call the defined function and get the class predictions of the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T11:21:27.740398Z",
     "iopub.status.busy": "2022-03-15T11:21:27.73974Z",
     "iopub.status.idle": "2022-03-15T11:21:56.133808Z",
     "shell.execute_reply": "2022-03-15T11:21:56.133096Z",
     "shell.execute_reply.started": "2022-03-15T11:21:27.74036Z"
    },
    "id": "jb7JmHhJyeJ5"
   },
   "outputs": [],
   "source": [
    "bert_preds = bert_predict(bert_classifier, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T11:21:56.13547Z",
     "iopub.status.busy": "2022-03-15T11:21:56.135225Z",
     "iopub.status.idle": "2022-03-15T11:21:56.159488Z",
     "shell.execute_reply": "2022-03-15T11:21:56.15868Z",
     "shell.execute_reply.started": "2022-03-15T11:21:56.135436Z"
    },
    "id": "v4TfBQbhyeJ5"
   },
   "outputs": [],
   "source": [
    "print('Classification Report for BERT :\\n', classification_report(y_test, bert_preds, target_names=sentiments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T11:21:56.161117Z",
     "iopub.status.busy": "2022-03-15T11:21:56.160856Z",
     "iopub.status.idle": "2022-03-15T11:21:56.700519Z",
     "shell.execute_reply": "2022-03-15T11:21:56.699884Z",
     "shell.execute_reply.started": "2022-03-15T11:21:56.161083Z"
    },
    "id": "kd3EMVvNyeJ6"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "cm=confusion_matrix(y_test,bert_preds)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HH1M_FwSgOiZ"
   },
   "outputs": [],
   "source": [
    "cm_display=ConfusionMatrixDisplay(cm)\n",
    "cm_display.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uwv44qGfyeJ6"
   },
   "source": [
    "**The performance scores of BERT Classifier are quite high and higher than those achieved using the LSTM model, with an overall accuracy around 95% and F1 scores well over 95%.**<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dBCnuz-5yeJ6"
   },
   "source": [
    "Thank your for checking out my notebook! Let me know if you have comments or if you want me to check out your work! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "srEjrGxXyeJ6"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0a4eed466e3a4a3281b7bd56891962e1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "17e9251916ec454397d1afd1452e3ea9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "20e7eee8c01d435eab7e076732ac9bab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_738205b4a64542a39c4c2520af16dab2",
      "placeholder": "",
      "style": "IPY_MODEL_e1f73c8646ee4c1086d211ce86ab003c",
      "value": "Downloading ()solve/main/vocab.txt: 100%"
     }
    },
    "20f59bc4ca3048a1baab3e77a45b62e8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "21ae65d1f376401c8f0f43e38eb9a6ab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0a4eed466e3a4a3281b7bd56891962e1",
      "placeholder": "",
      "style": "IPY_MODEL_3be7df48f6c145278d3954f026944d08",
      "value": " 570/570 [00:00&lt;00:00, 26.0kB/s]"
     }
    },
    "31911d3ac499476ab6b34813a651bed4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_437058f1482c43c0b9b300cbc4ddf86b",
      "placeholder": "",
      "style": "IPY_MODEL_3c90ee094a4d4feb8bb99dd3b0f7d0fd",
      "value": " 28.0/28.0 [00:00&lt;00:00, 1.30kB/s]"
     }
    },
    "3b1f16cbcc4344cc93f0d222fa305cc5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a95d58405a1d471dba15507f3fbd5398",
      "max": 440473133,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f06550b5dc3a498fa9a9f9a0e0fc21d3",
      "value": 440473133
     }
    },
    "3be7df48f6c145278d3954f026944d08": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3c90ee094a4d4feb8bb99dd3b0f7d0fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "40e35c80b02d4007bfb3e1159c17e78a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "437058f1482c43c0b9b300cbc4ddf86b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4374234b677147fca74cc0d342088e46": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4ebc1c3817f54e019d5557ae19814936": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_aa4897a5c3a747338f229135d54d13d6",
       "IPY_MODEL_ff3f4749c9dc4686be0629e823f2547e",
       "IPY_MODEL_31911d3ac499476ab6b34813a651bed4"
      ],
      "layout": "IPY_MODEL_e12bb278a6d74fd59254200e6c26aca1"
     }
    },
    "4effba97b2474449aa904e432cb45a8d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9c64220a271044fdb4f5b831ee4bb907",
       "IPY_MODEL_95939e89e4654a75afdd6b9affae2848",
       "IPY_MODEL_21ae65d1f376401c8f0f43e38eb9a6ab"
      ],
      "layout": "IPY_MODEL_83ada81d295c442989b34f6421d6ee52"
     }
    },
    "5e2238c0f4a04f2fa6a3c20ae7e5646b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "619135b67c814dcc8918e60ed9fc4ea5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "738205b4a64542a39c4c2520af16dab2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7af8778142244a4f956813e0b9b7db71": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_20f59bc4ca3048a1baab3e77a45b62e8",
      "placeholder": "",
      "style": "IPY_MODEL_619135b67c814dcc8918e60ed9fc4ea5",
      "value": "Downloading pytorch_model.bin: 100%"
     }
    },
    "7b1446638cf543deaf6646f85c822ba2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7b415fe6f4b349d897d29634516d959f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7bc393c41acb45cf8a3dd9dd3e51603f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "83ada81d295c442989b34f6421d6ee52": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "85fcd76fc9774fb5b5626ad17bc58c0a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8ebd4fc54ad74089bb9c6d65598f6cf6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "95939e89e4654a75afdd6b9affae2848": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4374234b677147fca74cc0d342088e46",
      "max": 570,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_bed3482961a7486bb0fdd1abb2f250dd",
      "value": 570
     }
    },
    "9c64220a271044fdb4f5b831ee4bb907": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_40e35c80b02d4007bfb3e1159c17e78a",
      "placeholder": "",
      "style": "IPY_MODEL_85fcd76fc9774fb5b5626ad17bc58c0a",
      "value": "Downloading ()lve/main/config.json: 100%"
     }
    },
    "a95d58405a1d471dba15507f3fbd5398": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aa4897a5c3a747338f229135d54d13d6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f8da253dc8c34bc28361bc92b0e0cc61",
      "placeholder": "",
      "style": "IPY_MODEL_df17dcfef6834e96a8ecfd355cf10f4e",
      "value": "Downloading ()okenizer_config.json: 100%"
     }
    },
    "ac46cacd630d410c8dad740229b79469": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_17e9251916ec454397d1afd1452e3ea9",
      "placeholder": "",
      "style": "IPY_MODEL_db53dc29a9414459ab2ad27cb963fc4f",
      "value": " 232k/232k [00:00&lt;00:00, 544kB/s]"
     }
    },
    "bed3482961a7486bb0fdd1abb2f250dd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d134fc4f02384d228bd998cf54917331": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d67636adc6d54b408874c07332589d93": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d8e556ad664a467ca4db1568fd192be6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_20e7eee8c01d435eab7e076732ac9bab",
       "IPY_MODEL_e67e18c7d4644fe3b7cf38451ed27377",
       "IPY_MODEL_ac46cacd630d410c8dad740229b79469"
      ],
      "layout": "IPY_MODEL_7b1446638cf543deaf6646f85c822ba2"
     }
    },
    "db53dc29a9414459ab2ad27cb963fc4f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "deefa33b61584260bd4b5bfbee1b9b1e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8ebd4fc54ad74089bb9c6d65598f6cf6",
      "placeholder": "",
      "style": "IPY_MODEL_d67636adc6d54b408874c07332589d93",
      "value": " 440M/440M [00:01&lt;00:00, 322MB/s]"
     }
    },
    "df17dcfef6834e96a8ecfd355cf10f4e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e12bb278a6d74fd59254200e6c26aca1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e1f73c8646ee4c1086d211ce86ab003c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e67e18c7d4644fe3b7cf38451ed27377": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e86b49099a464e79a02f56e2742ecbcb",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7bc393c41acb45cf8a3dd9dd3e51603f",
      "value": 231508
     }
    },
    "e755e5bc6b2b4933afca483a50911598": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7af8778142244a4f956813e0b9b7db71",
       "IPY_MODEL_3b1f16cbcc4344cc93f0d222fa305cc5",
       "IPY_MODEL_deefa33b61584260bd4b5bfbee1b9b1e"
      ],
      "layout": "IPY_MODEL_d134fc4f02384d228bd998cf54917331"
     }
    },
    "e86b49099a464e79a02f56e2742ecbcb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f06550b5dc3a498fa9a9f9a0e0fc21d3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f8da253dc8c34bc28361bc92b0e0cc61": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ff3f4749c9dc4686be0629e823f2547e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5e2238c0f4a04f2fa6a3c20ae7e5646b",
      "max": 28,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7b415fe6f4b349d897d29634516d959f",
      "value": 28
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
